{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ak5mqkCK8Hk",
        "outputId": "644ae8ba-1378-48f7-ce2b-ea43fbdfee02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "# Import Necessary Modules for Data Preprocessing\n",
        "\n",
        "# Used for loading in training data [Function #1 - load_raw_training_data()]\n",
        "import pandas as pd\n",
        "# Adjust column width settings to see all of the 'original_text' column\n",
        "pd.set_option('max_colwidth', 400)\n",
        "\n",
        "# Used for replacing '-LRB-' and '-RRB-' with left and right parentheses in original text repectively [Function #2 - replace_LRB_and_RRB()]\n",
        "import re\n",
        "\n",
        "# Used for label value changing in preprocessing training data [Function #6 - preprocessing_training_data()]\n",
        "import numpy as np\n",
        "\n",
        "# Used for tokenization when creating score values against extraneous resourses [Function #8 - extraneous_score_calculation()]\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "# Used for Parts-of-Speech tagging [Function #14 - POS_preprocessing()]\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "# Used for Lemmatization [Function #15 - lemma_preprocessing()]\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Used for vectorization [Function # ]\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Raw WikiLarge Training Data from GitHub Repository\n",
        "def load_raw_training_data():\n",
        "  # WikiLarge Training Data is very large and was split into three CSV files load each of them in.\n",
        "  textData_1 = pd.read_csv('https://raw.githubusercontent.com/nruloff/Difficulty_Classification_of_Textual_Passages/main/Data/WikiLarge_Train_part_1.csv')\n",
        "  textData_2 = pd.read_csv('https://raw.githubusercontent.com/nruloff/Difficulty_Classification_of_Textual_Passages/main/Data/WikiLarge_Train_part_2.csv')\n",
        "  textData_3 = pd.read_csv('https://raw.githubusercontent.com/nruloff/Difficulty_Classification_of_Textual_Passages/main/Data/WikiLarge_Train_part_3.csv')\n",
        "\n",
        "  # Concatenate each of the parts together to get the original data in one dataframe\n",
        "  text_data = pd.concat([textData_1, textData_2, textData_3], ignore_index=True)\n",
        "\n",
        "  # Return concatenated dataframe\n",
        "  return text_data"
      ],
      "metadata": {
        "id": "GiLkzoEnJ5Vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Replacing -LRB- and -RRB-**\n",
        "Notice the fifth entry in the 'original_text' column of the text_data dataframe. <br>\n",
        "![](https://drive.google.com/uc?id=1UCPuQrnQfSWAffp_XNTw916c8o4YgicV) <br> <br>\n",
        "Now Compare that with the wikipedia page regarding Geneva and the text of the first paragraph.\n",
        "![](https://drive.google.com/uc?id=1rxkFMR21K17lCM0-rOUL2x631se-fgxQ) <br> <br>\n",
        "Based off of this, we have determined that '-LRB- and '-RRB-' are the left and right parentheses respectively. "
      ],
      "metadata": {
        "id": "JWiY1moGRRNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After searching for the first few 'original_text' entries presented in text_data.head() - it was determined\n",
        "# that '-LRB-' and '-RRB-' are left and right parentheses respectively. This function replaces those\n",
        "# text strings with their respective symbols.\n",
        "def replace_LRB_and_RRB(text):\n",
        "  # Replace the substring \"-LRB-\" with \"(\" in input string\n",
        "  new_string = re.sub(\"-LRB-\", \"(\", text)\n",
        "\n",
        "  # Replace the substring \"-RRB-\" with \")\" in new_string\n",
        "  second_string = re.sub(\"-RRB-\", \")\", new_string)\n",
        "\n",
        "  # Return the output of the second replacement\n",
        "  return second_string"
      ],
      "metadata": {
        "id": "zZHKYx2cwnV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Based on the finding of '-LRB-' and '-RRB-', this function replaces every character in a string that is not a parentheses with no text\n",
        "def find_parentheses(text):\n",
        "  punctuation_string = obtain_non_Alphanumeric(text)\n",
        "  new_string = re.sub(\"[^()]\", \"\", punctuation_string)\n",
        "  return new_string"
      ],
      "metadata": {
        "id": "VxyIaFSm1BO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to find any uneven parentheses within the 'original_text' column\n",
        "def determine_uneven_parentheses(text):\n",
        "  # Use previously generated function to find all parentheses within the 'original_text' column\n",
        "  parentheses_only = find_parentheses(text)\n",
        "\n",
        "  # Generate a list containing a single string of a closed parentheses\n",
        "  parentheses_string_list = ['()']\n",
        "\n",
        "  # While any closed parentheses exist in the parentheses column\n",
        "  while any(x in parentheses_only for x in parentheses_string_list):\n",
        "    # Replace the closed parentheses with no text\n",
        "    for paren in parentheses_string_list:\n",
        "      parentheses_only = parentheses_only.replace(paren, \"\")\n",
        "\n",
        "  # Output result as a boolean to determine if string parentheses_only has been reduce to an empty string\n",
        "  result = not parentheses_only\n",
        "\n",
        "  # Return boolean value as 0 or 1 - 0 indicating that the 'original_text' column has closed parentheses\n",
        "  if result == False:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0"
      ],
      "metadata": {
        "id": "bmUiqlT68g_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to obtain any non-alphanumeric characters\n",
        "def obtain_non_Alphanumeric(text):\n",
        "  # replace all non-alphanumeric characters of an input string with no text to a new output called 'new_string'\n",
        "  new_string, number_of_subs = re.subn(\"[a-zA-Z0-9]\", \"\", text)\n",
        "\n",
        "  # replace all white space characters of 'new_string' with no text to the output 'second_string'\n",
        "  second_string, second_subs = re.subn(\"\\s\", \"\", new_string)\n",
        "\n",
        "  # return 'second_string'\n",
        "  return second_string"
      ],
      "metadata": {
        "id": "cS6PXpc-XQ8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Duplicate Values**\n",
        "In the WikiLarge training data - it was found that there were repeated entries of the 'original_text' column which had opposing label values. The following function takes all duplicate values and compared the total number of entries that label an 'original_text' sample as not requiring simplication against the total number of entries that label the sample as requiring simplification. The function then reassigns the duplicate values as a single entry with the more prominent label. If there is an even number of both labels, then the sample is not included in the post processed training data."
      ],
      "metadata": {
        "id": "GUUdjYGshvyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combined previous functions into large preprocessing function\n",
        "\n",
        "# Additionally - find duplicate 'original_text' entries, find the mean of their label values - as some of them have\n",
        "# opposing label values - then remove all duplicates except for one with the mean label value adjusted to 0 or 1\n",
        "# based on rounding\n",
        "def preprocessing_training_data():\n",
        "  text_data = load_raw_training_data()\n",
        "\n",
        "  # Convert binary 0 labels to -1 (this helps with keeping some of the duplicate entries by mean value calculation)\n",
        "  text_data['label'] = np.where(text_data['label'] < 1, -1, 1)\n",
        "\n",
        "  # Separate out the duplicate entries from the individual entries - All duplicate entries are taken to \"duplicate_texts\" dataframe\n",
        "  duplicate_texts = text_data[text_data.original_text.duplicated(keep=False)].copy()\n",
        "\n",
        "  # Individual entries can be used directly in the final version of the preprocessed dataframe\n",
        "  individual_texts = text_data[~text_data.original_text.duplicated(keep=False)].copy()\n",
        "  \n",
        "  # Group the duplicate text entries by the original text and find the mean value. \n",
        "  # If the mean value is negative, then most of the entries have been labeled as -1\n",
        "  # If the mean value is positive, then most of the entries have been labeled as 1\n",
        "  dup_group = duplicate_texts.groupby(['original_text'], as_index=False).mean()\n",
        "  \n",
        "  # Convert all positive values to 1 and all negative values to -1\n",
        "  dup_group['label'] = np.where(dup_group['label'] > 0, 1, dup_group['label'])\n",
        "  dup_group['label'] = np.where(dup_group['label'] < 0, -1, dup_group['label'])\n",
        "  \n",
        "  # If the label mean is 0, then it is an even split, and the text data cannot be used for classification\n",
        "  # Identify all rows with mean groupby label values of 0\n",
        "  zero_mean = dup_group[dup_group['label'] == 0]\n",
        "  \n",
        "  # Identify all rows with positive groupby label values\n",
        "  pos_mean = dup_group[dup_group['label'] > 0].copy()\n",
        "\n",
        "  # Perform the same for all rows with negative groupby label values\n",
        "  neg_mean = dup_group[dup_group['label'] < 0].copy()\n",
        "  \n",
        "  # Recombine the acceptable duplicate entries with the original individual entries\n",
        "  new_text_data = pd.concat([pos_mean, neg_mean, individual_texts], ignore_index=True)\n",
        "\n",
        "  # Convert the labels of -1 back to 0 as in the original training data\n",
        "  new_text_data['label'] = np.where(new_text_data['label'] < 0, 0, 1)\n",
        "\n",
        "  # Replace \"-LRB-\" and \"-RRB-\" with left and right parentheses\n",
        "  new_text_data['original_text'] = new_text_data.original_text.apply(lambda x: replace_LRB_and_RRB(x))\n",
        "\n",
        "  # Create a Column of only the punctuation using previously made function\n",
        "  new_text_data['punctuation'] = new_text_data.original_text.apply(lambda x: obtain_non_Alphanumeric(x))\n",
        "\n",
        "  # Determine if a text entry has closed parentheses or not\n",
        "  new_text_data['closed_parentheses'] = new_text_data.original_text.apply(lambda x: determine_uneven_parentheses(x))\n",
        "  \n",
        "  return new_text_data"
      ],
      "metadata": {
        "id": "MiEDmHTBKQkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function which loads external resouce data provided with the WikiLarge data - those resources are:\n",
        "# 1) The Dale Chall 3000 Word List, which is one definition of words that are considered \"basic\" English.\n",
        "# 2) \"Age of Acquisition\" (AoA) estimates for about 51k English words, which refers to the approximate age (in years) when a word was learned. Early words, being more basic, have lower average AoA.\n",
        "# 3) Brysbaert et al Concreteness Ratings for 40 thousand English lemma words gathered via \n",
        "#    Amazon Mechanical Turk. The ratings come from a larger list of 63 thousand words and represent all English words known to 85% of the raters.\n",
        "\n",
        "def load_external_resource_data():\n",
        "  # Load Dale Chall word list as a list of strings\n",
        "  dale_chall = pd.read_csv('https://raw.githubusercontent.com/nruloff/Difficulty_Classification_of_Textual_Passages/main/Data/dale_chall.txt', header=None)\n",
        "  d_c_df = dale_chall.rename(columns={0:'words'})\n",
        "  d_c_list = d_c_df['words'].to_list()\n",
        "\n",
        "  # Load AoA estimates for about 51 thousand English words, and return it as a dictionary\n",
        "  AoA = pd.read_csv('https://raw.githubusercontent.com/nruloff/Difficulty_Classification_of_Textual_Passages/main/Data/AoA_51715_words.csv', encoding='unicode_escape')\n",
        "  # Reduce the dataframe to the word and the AoA_Kup_lem score\n",
        "  AoA = AoA[['Word', 'AoA_Kup_lem']]\n",
        "  # Drop any rows where the AoA_Kup_lem score is not a value\n",
        "  AoA = AoA[AoA['AoA_Kup_lem'].notna()]\n",
        "  # Set the index of the dataframe to the words\n",
        "  AoA = AoA.set_index('Word')\n",
        "  # Take the AoA_Kup_lem score series out as a dictionary\n",
        "  AoA_dict = AoA['AoA_Kup_lem'].to_dict()\n",
        "\n",
        "  # Load Brysbaert Concreteness ratings, and return it as a dictionary\n",
        "  Brysbaert = pd.read_csv('https://raw.githubusercontent.com/nruloff/Difficulty_Classification_of_Textual_Passages/main/Data/Concreteness_ratings_Brysbaert_et_al_BRM.txt', delimiter='\\t')\n",
        "  # Reduce the dataframe to the word and Concreteness rating\n",
        "  Brysbaert = Brysbaert[['Word','Conc.M']]\n",
        "  # Remove any words that do not have a concreteness rating\n",
        "  Brysbaert = Brysbaert[Brysbaert['Conc.M'].notna()]\n",
        "  # Set the dataframe index to the word\n",
        "  Brysbaert = Brysbaert.set_index('Word')\n",
        "  # Take the Concreteness rating series out as a dictionary\n",
        "  Brysbaert_dict = Brysbaert['Conc.M'].to_dict()\n",
        "  \n",
        "  return d_c_list, AoA_dict, Brysbaert_dict"
      ],
      "metadata": {
        "id": "8FSDz9DIiwuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to calculate a score based on values from an external resource\n",
        "def extraneous_score_calculation(text, extraneous_dict):\n",
        "  # Replace all non-alphanumeric characters with a space, then make the letters lowercase, and \n",
        "  # subsequently tokenize the words\n",
        "  tokens = nltk.word_tokenize((re.sub(\"[^a-zA-Z0-9 ]\", \" \", text)).lower())\n",
        "  # Create an empty array to add score values into\n",
        "  score_array = []\n",
        "\n",
        "  # For each token in the tokenize 'original_text'\n",
        "  for tok in tokens:\n",
        "    # Try to find the token in the extraneous dictionary and append its score to the array\n",
        "    try:\n",
        "      ind_score = extraneous_dict[tok]\n",
        "      score_array.append(ind_score)\n",
        "    # If unable to find the token, append a value of 0 to the array\n",
        "    except:\n",
        "      score_array.append(0)\n",
        "  # Return a normalized score for the 'original_text' column by summing the scores together and dividing by \n",
        "  # the total number of tokens.\n",
        "  return np.sum(score_array)/len(score_array)"
      ],
      "metadata": {
        "id": "o_-MlFocOcl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use extraneous_score_calculation function to calculate AoA and Brysbaert Concreteness Scores\n",
        "def get_AoA_Brysbaert_features(new_text_data):\n",
        "  # Calculate AoA Score using extraneous_score_calculation function\n",
        "  new_text_data['AoA_score'] = new_text_data.original_text.apply(lambda x: extraneous_score_calculation(x, AoA_dict))\n",
        "\n",
        "  # Calculate Brysbaert Score using extraneous_score_calculation function\n",
        "  new_text_data['Brysbaert_score'] = new_text_data.original_text.apply(lambda x: extraneous_score_calculation(x, Brysbaert_dict))\n",
        "\n",
        "  # Convert NaN in Both 'AoA_score' and 'Brysbaert_score' columns\n",
        "  new_text_data['AoA_score'] = new_text_data['AoA_score'].fillna(0)\n",
        "  new_text_data['Brysbaert_score'] = new_text_data['Brysbaert_score'].fillna(0)\n",
        "\n",
        "  return new_text_data"
      ],
      "metadata": {
        "id": "QqVtMCSvQf7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Acquire additional features such as:\n",
        "# 1) Normalized proportion of word tokens from Dale Chall list in 'original_text' column\n",
        "# 2) Number of tokens in 'original_text' column\n",
        "# 3) Average length of each word token in 'original_text' column\n",
        "# 4) Largest length of a word token in 'original_text' column\n",
        "# 5) Normalized proportion of non-alphanumeric characters in 'original_text' column\n",
        "# 6) Normalized proportion of decimal digit characters in 'original_text' column\n",
        "def get_more_features(list_of_docs, easy_word_list):\n",
        "    # List for number of word tokens in text passage\n",
        "    num_toks_l = []\n",
        "    # List for number of dale_chall terms in text passage, normalized to length of text passage\n",
        "    d_c_norm_l = []\n",
        "    # Value of average word length for a textual passage\n",
        "    avg_tok_len_l = []\n",
        "    # Value of max word length for a textual passage\n",
        "    max_tok_len_l = []\n",
        "    # Number of Non-alphanumeric characters\n",
        "    non_alpha_char_l =[]\n",
        "    # Number of Characters total\n",
        "    numbers_norm_l = [] #add ratio of number charicters to total\n",
        "    # Generate a set of words based on the second input of the function (a list of words)\n",
        "    s2=set(easy_word_list)\n",
        "    \n",
        "    \n",
        "    for doc in list_of_docs:\n",
        "        # Convert all letters to lowercase\n",
        "        doc = doc.lower()\n",
        "        #-------------------\n",
        "        chars = re.findall('[^a-zA-Z0-9 ]', doc) # Find all non-alphanumeric characters (except whitespace)\n",
        "        non_alpha = len(chars)/len(doc) # Calculate a Normalized Ratio of the number of non-alphanumeric characters to the length of the entire text passage\n",
        "        non_alpha_char_l.append(non_alpha) # Append this ratio to the previously generated list\n",
        "        #-------------------------------\n",
        "        num_chars = re.findall('\\d', doc) # Find all decimal digit characters\n",
        "        numbers_norm = len(num_chars)/len(doc) # Calculate the normalized ratio to the length of the entire text passage\n",
        "        numbers_norm_l.append(numbers_norm) # Append the calculated ratio to previously generated list\n",
        "        #------------------------------\n",
        "        toks = nltk.word_tokenize(doc) # Generate word tokens for each text passage using nltk.tokenize.word_tokenize\n",
        "        num_toks = len(toks) # Count the number of tokens\n",
        "        num_toks_l.append(num_toks) # Append the token count to previously generated list\n",
        "        #------------------------------\n",
        "        temp_list = [] # Create an empty temporary list\n",
        "        # For each token created from word_tokenize\n",
        "        for tok in toks:\n",
        "            # Determine the length of the token, and append that length to the temporary list\n",
        "            temp_list.append(len(tok))\n",
        "            \n",
        "        # Find the average token length\n",
        "        avg_tok = sum(temp_list)/len(temp_list)\n",
        "        # Append the average token length to previously generated list\n",
        "        avg_tok_len_l.append(avg_tok)\n",
        "        # Find the maximum token length\n",
        "        max_t = max(temp_list)\n",
        "        # Append the maximum token length to previously generated list\n",
        "        max_tok_len_l.append(max_t)\n",
        "        #------------------------------\n",
        "        s1= set(toks) # Generate a set of tokens from previously made list of tokens\n",
        "        num_d_c = len(s1.intersection(s2)) # Calculate the number of words that are also contained in the set of 'simple words' made previously\n",
        "        d_c_norm = num_d_c/num_toks # Normalize the value to the total number of tokens\n",
        "        d_c_norm_l.append(d_c_norm) # Append that normalized value to previously generated list\n",
        "        #------------------------------\n",
        "        \n",
        "    # Generate an array of new features which can be added to the dataframe\n",
        "    new_features = np.vstack(( np.asarray(d_c_norm_l), np.asarray(num_toks_l),  np.asarray(avg_tok_len_l), np.asarray(max_tok_len_l), \n",
        "                   np.asarray(numbers_norm_l), np.asarray(non_alpha_char_l) )).T\n",
        "\n",
        "    return new_features"
      ],
      "metadata": {
        "id": "fh8dNoO_JNuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to count the number of a specific character within an 'original_text' column\n",
        "# This function designed to help with identifying non-alphanumeric characters as special characters\n",
        "# can have issue when using regex to search for them.\n",
        "def count_num_of_specific_char(text, char_of_interest):\n",
        "  # Reformat non-alphanumeric character as a set contained in brackets\n",
        "  reformat_char = '[' + char_of_interest + ']'\n",
        "  # Find all occurences of the character in a text, and count the total number of them\n",
        "  num_specific_char = len(re.findall(reformat_char, text))\n",
        "  # Return the total count of the non-alphanumeric characters\n",
        "  return num_specific_char"
      ],
      "metadata": {
        "id": "trJbKOjoSxf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to count the total number of non-whitespace characters\n",
        "def count_num_of_non_ws(text):\n",
        "  # Replace all whitespace characters with no text\n",
        "  new_string = re.sub(\"\\s\", \"\", text)\n",
        "\n",
        "  # Return the count the length of the new non-whitespace string\n",
        "  return len(new_string)"
      ],
      "metadata": {
        "id": "J6lHkfAduvVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make Individual feature columns for the normalized proportion of the punctuation/non-alphanumeric characters\n",
        "def get_punctuation_features(text_data):\n",
        "  # Acquire all of the non-alphanumeric characters in a set\n",
        "  all_punctuation = set(text_data.punctuation.sum())\n",
        "\n",
        "  # Create a new empty list to track all columns added to dataframe\n",
        "  new_columns = []\n",
        "\n",
        "  # For each non-alphanumeric character\n",
        "  for punc_mark in all_punctuation:\n",
        "    # Create a new string for a potential column name\n",
        "    new_col_name = 'norm_' + punc_mark\n",
        "\n",
        "    # Try to count the number of entries of the specific character, and if so, add the name of the column to the list of new column names\n",
        "    try:\n",
        "      text_data[new_col_name] = text_data.punctuation.apply(lambda x: count_num_of_specific_char(x, punc_mark))\n",
        "      text_data[new_col_name] = text_data[new_col_name] / text_data['num_non_ws_char']\n",
        "      new_columns.append(new_col_name)\n",
        "    # If there are issues, then continue to the next non-alphanumeric character\n",
        "    except:\n",
        "      continue\n",
        "  # Return the dataframe and a list of these new columns\n",
        "  return text_data, new_columns"
      ],
      "metadata": {
        "id": "qN_RMn7_Sawa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to transform a text into an array of Parts-of-Speech (POS)\n",
        "def POS_preprocessing(text):\n",
        "  # Replace all non-alphanumeric or common punctuation with no text and output into a new string\n",
        "  new_string = re.sub('[^a-zA-Z0-9 ,.!;:?()]', '', text)\n",
        "  # Tokenize the new string\n",
        "  word_tokens = nltk.word_tokenize(new_string)\n",
        "  # Obtain the parts of speech tags for each of the words and put them into a list\n",
        "  pos_tag_tokens = [pair[1] for pair in nltk.pos_tag(word_tokens)]\n",
        "  # Concatenate each POS tag together into a single string\n",
        "  pos_tag_tokens = \" \".join(pos_tag_tokens)\n",
        "  # Return the single string of POS tags\n",
        "  return pos_tag_tokens"
      ],
      "metadata": {
        "id": "gJhL0ZrC46pT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatize and tokenize text\n",
        "def lemma_preprocessing(text):\n",
        "  lemmatizer = nltk.WordNetLemmatizer()\n",
        "  text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "  text = text.lower()\n",
        "  text = nltk.word_tokenize(text)\n",
        "  text = [lemmatizer.lemmatize(word) for word in text]\n",
        "  text = \" \".join(text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "vwLw6QL_2JdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_text_data = preprocessing_training_data()\n",
        "d_c_list, AoA_dict, Brysbaert_dict = load_external_resource_data()\n",
        "new_text_data = get_AoA_Brysbaert_features(new_text_data)\n",
        "new_features = get_more_features(new_text_data['original_text'], d_c_list)\n",
        "new_text_data[['d_c_norm_1', 'num_toks_1', 'avg_tok_len_1', 'max_tok_len_1', 'num_char_norm_1', 'non_alphanumeric_1']] = new_features\n",
        "new_text_data['num_non_ws_char'] = new_text_data['original_text'].apply(lambda x: count_num_of_non_ws(x))\n",
        "new_text_data, punc_cols = get_punctuation_features(new_text_data)\n",
        "new_text_data['pos_tag_tokens'] = new_text_data['original_text'].apply(lambda x: POS_preprocessing(x))\n",
        "new_text_data['num_pos_tokens'] = new_text_data['pos_tag_tokens'].apply(lambda x: len(x.split()))\n",
        "new_text_data['lemma_text'] = new_text_data['original_text'].apply(lambda x: lemma_preprocessing(x))\n",
        "new_text_data = new_text_data.drop(columns=['punctuation'])\n",
        "#new_text_data.head()"
      ],
      "metadata": {
        "id": "Fa53w2AeLCES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4a75740-f97e-4ae0-a396-14eb481de81c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-cc2f911ccd95>:8: FutureWarning: Possible nested set at position 1\n",
            "  num_specific_char = len(re.findall(reformat_char, text))\n",
            "<ipython-input-14-60cef8cc6f5e>:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  text_data[new_col_name] = text_data.punctuation.apply(lambda x: count_num_of_specific_char(x, punc_mark))\n",
            "<ipython-input-17-15b3d35ea10b>:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  new_text_data['pos_tag_tokens'] = new_text_data['original_text'].apply(lambda x: POS_preprocessing(x))\n",
            "<ipython-input-17-15b3d35ea10b>:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  new_text_data['num_pos_tokens'] = new_text_data['pos_tag_tokens'].apply(lambda x: len(x.split()))\n",
            "<ipython-input-17-15b3d35ea10b>:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  new_text_data['lemma_text'] = new_text_data['original_text'].apply(lambda x: lemma_preprocessing(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def determine_X_feat(df):\n",
        "  all_features = df.columns.to_list()\n",
        "  X_feat = []\n",
        "  for feat in all_features:\n",
        "    if feat != 'label':\n",
        "      X_feat.append(feat)\n",
        "  return X_feat"
      ],
      "metadata": {
        "id": "tpP_GDy--cNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler\n",
        "\n",
        "def scikit_column_transformer(text_df = new_text_data, text_type = 'original_text', vector_type = 'Count', scaler='Robust', ngrams_value=1, max_features_value=None, sequence_length=500, \n",
        "                              test_size=0.2, random_state=21):\n",
        "  # Reduce the input dataframe to only include either the original_text or lemma_text columns\n",
        "  if text_type == 'original_text':\n",
        "    final_text_df = text_df.drop(columns=['lemma_text'])\n",
        "    final_text_df = final_text_df.rename(columns={'original_text': 'text'})\n",
        "  elif text_type == 'lemma_text':\n",
        "    final_text_df = text_df.drop(columns=['original_text'])\n",
        "    final_text_df = final_text_df.rename(columns={'lemma_text': 'text'})\n",
        "  else:\n",
        "    return 'Incorrect input for text_type argument'\n",
        "\n",
        "  # Perform the Train-Test Split Based on Input Data\n",
        "  X_feat = determine_X_feat(final_text_df)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(final_text_df[X_feat], final_text_df['label'], test_size=test_size, random_state=random_state)\n",
        "  \n",
        "  # Select Vectors for text data and POS data\n",
        "  if vector_type == 'Count':\n",
        "    text_vector = CountVectorizer(ngram_range=(1, ngrams_value),max_features=max_features_value)\n",
        "    pos_vector = CountVectorizer(ngram_range=(1, ngrams_value), preprocessor=None, token_pattern=r'[^\\s]+', lowercase=False)\n",
        "  elif vector_type == 'Tfidf':\n",
        "    text_vector = TfidfVectorizer(ngram_range=(1, ngrams_value), max_features=max_features_value)\n",
        "    pos_vector = TfidfVectorizer(ngram_range=(1, ngrams_value), token_pattern=r'[^\\s]+', lowercase=False)\n",
        "  elif vector_type == 'Binary':\n",
        "    text_vector = CountVectorizer(binary=True, ngram_range=(1, ngrams_value),max_features=max_features_value)\n",
        "    pos_vector = CountVectorizer(binary=True, ngram_range=(1, ngrams_value), preprocessor=None, token_pattern=r'[^\\s]+', lowercase=False)\n",
        "  else:\n",
        "    return 'Incorrect input for vector_type argument'\n",
        "\n",
        "  # Select the desired scaler based on input string\n",
        "  dict_of_scalers = {'Robust': RobustScaler(), 'MinMax': MinMaxScaler() , 'Standard': StandardScaler()}\n",
        "  try:\n",
        "    selected_feature_scaler = dict_of_scalers[scaler]\n",
        "  except:\n",
        "    return 'Incorrect input for scaler argument - must be either Count, MinMax or Standard'\n",
        "  \n",
        "  # Use Scikit-Learn Column Transformer to vectorize the text data and the POS data, and transform the additional features by selected scaler\n",
        "  column_trans = ColumnTransformer([('vector_text', text_vector, 'text'), \n",
        "                                    ('vector_pos_tags', pos_vector, 'pos_tag_tokens')], \n",
        "                                   remainder = selected_feature_scaler)\n",
        "  \n",
        "  # Perform Fit_Transform on X_train and transform on X_test\n",
        "  X_train_matrix = column_trans.fit_transform(X_train)\n",
        "  X_test_matrix = column_trans.transform(X_test)\n",
        "\n",
        "  return column_trans, X_train_matrix, y_train, X_test_matrix, y_test\n"
      ],
      "metadata": {
        "id": "UkS-AlHeKXLU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}