{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IX_gj1cyYA1F"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "feature_ablation_dict_files = ['overall_dict_23Feb2023.pkl', 'overall_dict_23Feb2023_Part_2.pkl', \n",
        "                               'overall_dict_24Feb2023_Part_1.pkl', 'overall_dict_24Feb2023_Part_2.pkl', \n",
        "                               'overall_dict_25Feb2023_Part_1.pkl']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Generate the base file path name for loading the pickle file\n",
        "base_path = '/content/drive/My Drive/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9Itk14fdetG",
        "outputId": "e9fd9180-471b-4ec2-93c2-c616c5c31167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Necessary Modules for Data Preprocessing\n",
        "\n",
        "# Used for loading in training data [Function #1 - load_raw_training_data()]\n",
        "import pandas as pd\n",
        "# Adjust column width settings to see all of the 'original_text' column\n",
        "pd.set_option('max_colwidth', 400)\n",
        "\n",
        "# Used for replacing '-LRB-' and '-RRB-' with left and right parentheses in original text repectively [Function #2 - replace_LRB_and_RRB()]\n",
        "import re\n",
        "\n",
        "# Used for label value changing in preprocessing training data [Function #6 - preprocessing_training_data()]\n",
        "import numpy as np\n",
        "\n",
        "# Used for tokenization when creating score values against extraneous resourses [Function #8 - extraneous_score_calculation()]\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "# Used for Parts-of-Speech tagging [Function #14 - POS_preprocessing()]\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "# Used for Lemmatization [Function #15 - lemma_preprocessing()]\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Used for vectorization [Function # ]\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SL8nEMyQeZeK",
        "outputId": "640d68b4-c226-4348-9dc6-1341346284cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "recombined_dictionary = {}\n",
        "\n",
        "for overall_dict_file in feature_ablation_dict_files:\n",
        "  overall_dict_file = base_path + overall_dict_file\n",
        "  with open(overall_dict_file, 'rb') as handle:\n",
        "    temp_dict = pickle.load(handle)\n",
        "    key_list = list(temp_dict.keys())\n",
        "    for _key in key_list:\n",
        "      inner_list = [temp_dict[_key]['train_acc'], \n",
        "                    temp_dict[_key]['train_precision'], \n",
        "                    temp_dict[_key]['train_recall'], \n",
        "                    temp_dict[_key]['train_f1'], \n",
        "                    temp_dict[_key]['train_roc_auc_value'], \n",
        "                    temp_dict[_key]['test_acc'], \n",
        "                    temp_dict[_key]['test_precision'], \n",
        "                    temp_dict[_key]['test_recall'], \n",
        "                    temp_dict[_key]['test_f1'], \n",
        "                    temp_dict[_key]['test_roc_auc_value']]\n",
        "      recombined_dictionary[_key] = inner_list"
      ],
      "metadata": {
        "id": "QUU7t29Vbu2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "fE6-2tBabvFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_ablation_df = pd.DataFrame.from_dict(recombined_dictionary, orient='index', \n",
        "                                             columns=['train_acc', 'train_precision', 'train_recall', \n",
        "                                                      'train_f1', 'train_roc_auc_value', \n",
        "                                                      'test_acc', 'test_precision', 'test_recall', \n",
        "                                                      'test_f1', 'test_roc_auc_value'])\n",
        "feature_ablation_df.loc['all_columns'] = [0.7086740443237308, 0.7021821858368557, \n",
        "                                          0.7233819402250962, \n",
        "                                          0.71262443084246, 0.7873256955384904, \n",
        "                                          0.6982287844202375, 0.689122548463893, 0.7102588070339428, \n",
        "                                          0.6995310567047384, 0.7708407927116426]\n",
        "feature_ablation_df = feature_ablation_df.sort_values(['test_acc', 'test_f1'], ascending=False)\n",
        "feature_ablation_df.iloc[114]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2T5t-y6bvH4",
        "outputId": "46f06b0e-3f6f-4164-f1b8-248a30b4970b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "train_acc              0.708674\n",
              "train_precision        0.702182\n",
              "train_recall           0.723382\n",
              "train_f1               0.712624\n",
              "train_roc_auc_value    0.787326\n",
              "test_acc               0.698229\n",
              "test_precision         0.689123\n",
              "test_recall            0.710259\n",
              "test_f1                0.699531\n",
              "test_roc_auc_value     0.770841\n",
              "Name: all_columns, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_ablation_df.iloc[:113].index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvVT5QIHwW55",
        "outputId": "f7afaa41-1c41-4b31-8d29-3c960353a9d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['norm_.', 'norm_;', 'norm_/', 'norm__', 'Brysbaert_score',\n",
              "       'non_alphanumeric_1', 'avg_tok_len_1', 'norm_:', 'num_pos_tokens',\n",
              "       'norm_)',\n",
              "       ...\n",
              "       'norm_ü', 'norm_è', 'norm_œ', 'norm_Š', 'norm_•', 'norm_+', 'norm_#',\n",
              "       'norm_!', 'norm_Ò', 'norm_Ç'],\n",
              "      dtype='object', length=113)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(feature_ablation_df.iloc[:113].index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6rlHQxfU4FS",
        "outputId": "0d677149-8d76-4491-d9a0-2fd1b71d2e46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "113"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the base file path name for loading the pickle file\n",
        "base_path = '/content/drive/My Drive/new_text_data.pkl'\n",
        "new_text_data = pd.read_pickle(base_path)"
      ],
      "metadata": {
        "id": "CUoxmZKVwnPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def determine_X_feat(df):\n",
        "  all_features = df.columns.to_list()\n",
        "  X_feat = []\n",
        "  for feat in all_features:\n",
        "    if feat != 'label':\n",
        "      X_feat.append(feat)\n",
        "  return X_feat"
      ],
      "metadata": {
        "id": "g8x1xDjhwrOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def determine_engineered_feat(df):\n",
        "  X_feat = determine_X_feat(df)\n",
        "\n",
        "  engineered_features = []\n",
        "\n",
        "  for feat in X_feat:\n",
        "    if feat != 'original_text':\n",
        "      if feat != 'lemma_text':\n",
        "        if feat != 'text':\n",
        "          if feat!= 'pos_tag_tokens':\n",
        "            engineered_features.append(feat)\n",
        "  return engineered_features"
      ],
      "metadata": {
        "id": "jSAzaENtwsan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler\n",
        "\n",
        "def scikit_column_transformer(text_df = new_text_data, text_type = 'original_text', vector_type = 'Count', scaler='Robust', ngrams_value=1, max_features_value=None, sequence_length=500, \n",
        "                              test_size=0.2, random_state=21):\n",
        "  # Reduce the input dataframe to only include either the original_text or lemma_text columns\n",
        "  if text_type == 'original_text':\n",
        "    final_text_df = text_df.drop(columns=['lemma_text'])\n",
        "    final_text_df = final_text_df.rename(columns={'original_text': 'text'})\n",
        "  elif text_type == 'lemma_text':\n",
        "    final_text_df = text_df.drop(columns=['original_text'])\n",
        "    final_text_df = final_text_df.rename(columns={'lemma_text': 'text'})\n",
        "  else:\n",
        "    return 'Incorrect input for text_type argument'\n",
        "\n",
        "  # Perform the Train-Test Split Based on Input Data\n",
        "  X_feat = determine_X_feat(final_text_df)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(final_text_df[X_feat], final_text_df['label'], test_size=test_size, random_state=random_state)\n",
        "  \n",
        "  # Select Vectors for text data and POS data\n",
        "  if vector_type == 'Count':\n",
        "    text_vector = CountVectorizer(ngram_range=(1, ngrams_value),max_features=max_features_value)\n",
        "    pos_vector = CountVectorizer(ngram_range=(1, ngrams_value), preprocessor=None, token_pattern=r'[^\\s]+', lowercase=False)\n",
        "  elif vector_type == 'Tfidf':\n",
        "    text_vector = TfidfVectorizer(ngram_range=(1, ngrams_value), max_features=max_features_value)\n",
        "    pos_vector = TfidfVectorizer(ngram_range=(1, ngrams_value), token_pattern=r'[^\\s]+', lowercase=False)\n",
        "  elif vector_type == 'Binary':\n",
        "    text_vector = CountVectorizer(binary=True, ngram_range=(1, ngrams_value),max_features=max_features_value)\n",
        "    pos_vector = CountVectorizer(binary=True, ngram_range=(1, ngrams_value), preprocessor=None, token_pattern=r'[^\\s]+', lowercase=False)\n",
        "  else:\n",
        "    return 'Incorrect input for vector_type argument'\n",
        "\n",
        "  # Select the desired scaler based on input string\n",
        "  dict_of_scalers = {'Robust': RobustScaler(), 'MinMax': MinMaxScaler() , 'Standard': StandardScaler()}\n",
        "  try:\n",
        "    selected_feature_scaler = dict_of_scalers[scaler]\n",
        "  except:\n",
        "    return 'Incorrect input for scaler argument - must be either Count, MinMax or Standard'\n",
        "  \n",
        "  # Use Scikit-Learn Column Transformer to vectorize the text data and the POS data, and transform the additional features by selected scaler\n",
        "  column_trans = ColumnTransformer([('vector_text', text_vector, 'text'), \n",
        "                                    ('vector_pos_tags', pos_vector, 'pos_tag_tokens')], \n",
        "                                   remainder = selected_feature_scaler)\n",
        "  \n",
        "  # Perform Fit_Transform on X_train and transform on X_test\n",
        "  X_train_matrix = column_trans.fit_transform(X_train)\n",
        "  X_test_matrix = column_trans.transform(X_test)\n",
        "\n",
        "  return column_trans, X_train_matrix, y_train, X_test_matrix, y_test"
      ],
      "metadata": {
        "id": "Doce9imtwsha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def obtain_comparison_metrics(y_true, y_pred):\n",
        "    calc_accuracy = metrics.accuracy_score(y_true, y_pred)\n",
        "    calc_precision = metrics.precision_score(y_true, y_pred)\n",
        "    calc_recall = metrics.recall_score(y_true, y_pred)\n",
        "    calc_f1 = metrics.f1_score(y_true, y_pred)\n",
        "    \n",
        "    return calc_accuracy, calc_precision, calc_recall, calc_f1\n",
        "\n",
        "def obtain_train_and_test_metrics(y_train_true, y_train_pred, y_test_true, y_test_pred):\n",
        "    (calc_train_accuracy, calc_train_precision, \n",
        "     calc_train_recall, calc_train_f1) = obtain_comparison_metrics(y_train_true, y_train_pred)\n",
        "    (calc_test_accuracy, calc_test_precision, \n",
        "     calc_test_recall, calc_test_f1) = obtain_comparison_metrics(y_test_true, y_test_pred)\n",
        "    output_dict = {'train_acc': calc_train_accuracy, \n",
        "                   'train_precision': calc_train_precision, \n",
        "                   'train_recall': calc_train_recall, \n",
        "                   'train_f1': calc_train_f1, \n",
        "                   'test_acc': calc_test_accuracy, \n",
        "                   'test_precision': calc_test_precision, \n",
        "                   'test_recall': calc_test_recall, \n",
        "                   'test_f1': calc_test_f1}\n",
        "    return output_dict\n",
        "\n",
        "def obtain_roc_auc_score(clf, X, y_true):\n",
        "    y_score = clf.predict_proba(X)[:, 1]\n",
        "    roc_auc_value = metrics.roc_auc_score(y_true.values, y_score)\n",
        "    return roc_auc_value"
      ],
      "metadata": {
        "id": "6uY20nLewz1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import time"
      ],
      "metadata": {
        "id": "SrmwOB43wz4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_to_be_ablated = feature_ablation_df.iloc[:113].index.to_list()\n",
        "new_text_data = new_text_data.drop(columns=features_to_be_ablated)"
      ],
      "metadata": {
        "id": "Q3WIdzkrwNZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def info_gathering_repeat(input_text_df):\n",
        "  tfidf_trans, X_train_tfidf, y_train_tfidf, X_test_tfidf, y_test_tfidf = scikit_column_transformer(text_df = input_text_df, text_type = 'original_text', vector_type = 'Tfidf')\n",
        "  clf = GradientBoostingClassifier(n_estimators=n_estimators_val, learning_rate=learning_rate_val, random_state=0)\n",
        "  clf.fit(X_train_tfidf, y_train_tfidf)\n",
        "  train_preds = clf.predict(X_train_tfidf)\n",
        "  test_preds = clf.predict(X_test_tfidf)\n",
        "  output_dict = obtain_train_and_test_metrics(y_train_tfidf, train_preds, y_test_tfidf, test_preds)\n",
        "  output_dict['train_roc_auc_value'] = obtain_roc_auc_score(clf, X_train_tfidf, y_train_tfidf)\n",
        "  output_dict['test_roc_auc_value'] = obtain_roc_auc_score(clf, X_test_tfidf, y_test_tfidf)\n",
        "  output_dict['columns_in_data'] = input_text_df.columns.to_list()\n",
        "  output_dict['feature_importances'] = clf.feature_importances_\n",
        "  return output_dict"
      ],
      "metadata": {
        "id": "gqmvwE3-w3cQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_estimators_val = 100\n",
        "learning_rate_val = 1\n",
        "engineered_features = determine_engineered_feat(new_text_data)"
      ],
      "metadata": {
        "id": "czfPlcnFybqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "info_gathering_repeat(new_text_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xFCrVRTw3fJ",
        "outputId": "eeaddd8e-dbf6-4546-a446-7cd57b61f235"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train_acc': 0.7085981970007802,\n",
              " 'train_precision': 0.7011101562882075,\n",
              " 'train_recall': 0.7258629169921592,\n",
              " 'train_f1': 0.7132718517333901,\n",
              " 'test_acc': 0.6976075587274986,\n",
              " 'test_precision': 0.6873292590193483,\n",
              " 'test_recall': 0.7128877723900217,\n",
              " 'test_f1': 0.6998752527207812,\n",
              " 'train_roc_auc_value': 0.7861819293888062,\n",
              " 'test_roc_auc_value': 0.7695065107155004,\n",
              " 'columns_in_data': ['original_text',\n",
              "  'label',\n",
              "  'closed_parentheses',\n",
              "  'AoA_score',\n",
              "  'd_c_norm_1',\n",
              "  'num_toks_1',\n",
              "  'max_tok_len_1',\n",
              "  'num_char_norm_1',\n",
              "  'num_non_ws_char',\n",
              "  'norm_í',\n",
              "  'norm_(',\n",
              "  'norm_=',\n",
              "  \"norm_'\",\n",
              "  'norm_,',\n",
              "  'norm_-',\n",
              "  'pos_tag_tokens',\n",
              "  'lemma_text'],\n",
              " 'feature_importances': array([0.00000000e+00, 6.16293000e-05, 0.00000000e+00, ...,\n",
              "        5.22771301e-03, 5.81694491e-03, 3.05458334e-03])}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_ablation_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "LZNspGgew3iI",
        "outputId": "5ac992f7-a6a4-423a-fc43-0765782276d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 train_acc  train_precision  train_recall  train_f1  \\\n",
              "norm_.            0.708652         0.703251      0.720597  0.711819   \n",
              "norm_;            0.710870         0.703861      0.726731  0.715113   \n",
              "norm_/            0.709349         0.703328      0.722818  0.712940   \n",
              "norm__            0.709985         0.703335      0.725002  0.714004   \n",
              "Brysbaert_score   0.709750         0.703046      0.724923  0.713817   \n",
              "...                    ...              ...           ...       ...   \n",
              "norm_,            0.709859         0.704179      0.722434  0.713190   \n",
              "num_char_norm_1   0.709353         0.701602      0.727230  0.714186   \n",
              "norm_=            0.709342         0.701974      0.726239  0.713900   \n",
              "norm_-            0.709010         0.703496      0.721219  0.712247   \n",
              "d_c_norm_1        0.709100         0.703104      0.722521  0.712680   \n",
              "\n",
              "                 train_roc_auc_value  test_acc  test_precision  test_recall  \\\n",
              "norm_.                      0.786238  0.699948        0.691684     0.709645   \n",
              "norm_;                      0.788354  0.699587        0.689755     0.713530   \n",
              "norm_/                      0.787793  0.699428        0.690954     0.709704   \n",
              "norm__                      0.788093  0.699327        0.690167     0.711456   \n",
              "Brysbaert_score             0.787536  0.699255        0.690113     0.711340   \n",
              "...                              ...       ...             ...          ...   \n",
              "norm_,                      0.786901  0.697059        0.688536     0.707542   \n",
              "num_char_norm_1             0.787255  0.696943        0.687103     0.711047   \n",
              "norm_=                      0.786468  0.696582        0.686387     0.711690   \n",
              "norm_-                      0.786917  0.696538        0.687853     0.707484   \n",
              "d_c_norm_1                  0.787202  0.695946        0.687532     0.706169   \n",
              "\n",
              "                  test_f1  test_roc_auc_value  \n",
              "norm_.           0.700549            0.770535  \n",
              "norm_;           0.701442            0.772927  \n",
              "norm_/           0.700203            0.771891  \n",
              "norm__           0.700650            0.771641  \n",
              "Brysbaert_score  0.700565            0.771178  \n",
              "...                   ...                 ...  \n",
              "norm_,           0.697910            0.770490  \n",
              "num_char_norm_1  0.698870            0.770077  \n",
              "norm_=           0.698810            0.769545  \n",
              "norm_-           0.697530            0.770273  \n",
              "d_c_norm_1       0.696726            0.769615  \n",
              "\n",
              "[127 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b02ac987-922f-4db3-a865-c0028e9ee806\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_acc</th>\n",
              "      <th>train_precision</th>\n",
              "      <th>train_recall</th>\n",
              "      <th>train_f1</th>\n",
              "      <th>train_roc_auc_value</th>\n",
              "      <th>test_acc</th>\n",
              "      <th>test_precision</th>\n",
              "      <th>test_recall</th>\n",
              "      <th>test_f1</th>\n",
              "      <th>test_roc_auc_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>norm_.</th>\n",
              "      <td>0.708652</td>\n",
              "      <td>0.703251</td>\n",
              "      <td>0.720597</td>\n",
              "      <td>0.711819</td>\n",
              "      <td>0.786238</td>\n",
              "      <td>0.699948</td>\n",
              "      <td>0.691684</td>\n",
              "      <td>0.709645</td>\n",
              "      <td>0.700549</td>\n",
              "      <td>0.770535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>norm_;</th>\n",
              "      <td>0.710870</td>\n",
              "      <td>0.703861</td>\n",
              "      <td>0.726731</td>\n",
              "      <td>0.715113</td>\n",
              "      <td>0.788354</td>\n",
              "      <td>0.699587</td>\n",
              "      <td>0.689755</td>\n",
              "      <td>0.713530</td>\n",
              "      <td>0.701442</td>\n",
              "      <td>0.772927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>norm_/</th>\n",
              "      <td>0.709349</td>\n",
              "      <td>0.703328</td>\n",
              "      <td>0.722818</td>\n",
              "      <td>0.712940</td>\n",
              "      <td>0.787793</td>\n",
              "      <td>0.699428</td>\n",
              "      <td>0.690954</td>\n",
              "      <td>0.709704</td>\n",
              "      <td>0.700203</td>\n",
              "      <td>0.771891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>norm__</th>\n",
              "      <td>0.709985</td>\n",
              "      <td>0.703335</td>\n",
              "      <td>0.725002</td>\n",
              "      <td>0.714004</td>\n",
              "      <td>0.788093</td>\n",
              "      <td>0.699327</td>\n",
              "      <td>0.690167</td>\n",
              "      <td>0.711456</td>\n",
              "      <td>0.700650</td>\n",
              "      <td>0.771641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Brysbaert_score</th>\n",
              "      <td>0.709750</td>\n",
              "      <td>0.703046</td>\n",
              "      <td>0.724923</td>\n",
              "      <td>0.713817</td>\n",
              "      <td>0.787536</td>\n",
              "      <td>0.699255</td>\n",
              "      <td>0.690113</td>\n",
              "      <td>0.711340</td>\n",
              "      <td>0.700565</td>\n",
              "      <td>0.771178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>norm_,</th>\n",
              "      <td>0.709859</td>\n",
              "      <td>0.704179</td>\n",
              "      <td>0.722434</td>\n",
              "      <td>0.713190</td>\n",
              "      <td>0.786901</td>\n",
              "      <td>0.697059</td>\n",
              "      <td>0.688536</td>\n",
              "      <td>0.707542</td>\n",
              "      <td>0.697910</td>\n",
              "      <td>0.770490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>num_char_norm_1</th>\n",
              "      <td>0.709353</td>\n",
              "      <td>0.701602</td>\n",
              "      <td>0.727230</td>\n",
              "      <td>0.714186</td>\n",
              "      <td>0.787255</td>\n",
              "      <td>0.696943</td>\n",
              "      <td>0.687103</td>\n",
              "      <td>0.711047</td>\n",
              "      <td>0.698870</td>\n",
              "      <td>0.770077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>norm_=</th>\n",
              "      <td>0.709342</td>\n",
              "      <td>0.701974</td>\n",
              "      <td>0.726239</td>\n",
              "      <td>0.713900</td>\n",
              "      <td>0.786468</td>\n",
              "      <td>0.696582</td>\n",
              "      <td>0.686387</td>\n",
              "      <td>0.711690</td>\n",
              "      <td>0.698810</td>\n",
              "      <td>0.769545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>norm_-</th>\n",
              "      <td>0.709010</td>\n",
              "      <td>0.703496</td>\n",
              "      <td>0.721219</td>\n",
              "      <td>0.712247</td>\n",
              "      <td>0.786917</td>\n",
              "      <td>0.696538</td>\n",
              "      <td>0.687853</td>\n",
              "      <td>0.707484</td>\n",
              "      <td>0.697530</td>\n",
              "      <td>0.770273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d_c_norm_1</th>\n",
              "      <td>0.709100</td>\n",
              "      <td>0.703104</td>\n",
              "      <td>0.722521</td>\n",
              "      <td>0.712680</td>\n",
              "      <td>0.787202</td>\n",
              "      <td>0.695946</td>\n",
              "      <td>0.687532</td>\n",
              "      <td>0.706169</td>\n",
              "      <td>0.696726</td>\n",
              "      <td>0.769615</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>127 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b02ac987-922f-4db3-a865-c0028e9ee806')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b02ac987-922f-4db3-a865-c0028e9ee806 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b02ac987-922f-4db3-a865-c0028e9ee806');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_text_data = pd.read_pickle(base_path)\n",
        "new_text_data = new_text_data.drop(columns=['norm_.', 'norm_;'])\n",
        "test = info_gathering_repeat(new_text_data)\n",
        "test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdiKIQAO3BG9",
        "outputId": "a88c212a-d84e-4a28-bdbd-965ea2b6a7eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train_acc': 0.7092447051345026,\n",
              " 'train_precision': 0.702458246273366,\n",
              " 'train_recall': 0.7246622110349218,\n",
              " 'train_f1': 0.7133874975967872,\n",
              " 'test_acc': 0.698618856366841,\n",
              " 'test_precision': 0.6881877796853452,\n",
              " 'test_recall': 0.7142606765204182,\n",
              " 'test_f1': 0.7009818676987026,\n",
              " 'train_roc_auc_value': 0.7867076714681589,\n",
              " 'test_roc_auc_value': 0.7700709793592988,\n",
              " 'columns_in_data': ['original_text',\n",
              "  'label',\n",
              "  'closed_parentheses',\n",
              "  'AoA_score',\n",
              "  'Brysbaert_score',\n",
              "  'd_c_norm_1',\n",
              "  'num_toks_1',\n",
              "  'avg_tok_len_1',\n",
              "  'max_tok_len_1',\n",
              "  'num_char_norm_1',\n",
              "  'non_alphanumeric_1',\n",
              "  'num_non_ws_char',\n",
              "  'norm_<',\n",
              "  'norm_ß',\n",
              "  'norm_Ð',\n",
              "  'norm_º',\n",
              "  'norm_ù',\n",
              "  'norm_™',\n",
              "  'norm_Ò',\n",
              "  'norm_Ç',\n",
              "  'norm_ë',\n",
              "  'norm_å',\n",
              "  'norm_ø',\n",
              "  'norm_>',\n",
              "  'norm_Ö',\n",
              "  'norm_ž',\n",
              "  'norm_Õ',\n",
              "  'norm_Í',\n",
              "  'norm_ï',\n",
              "  'norm_Ë',\n",
              "  'norm_Ã',\n",
              "  'norm_â',\n",
              "  'norm_Ì',\n",
              "  'norm_í',\n",
              "  'norm_Ô',\n",
              "  'norm_þ',\n",
              "  'norm_§',\n",
              "  'norm_¹',\n",
              "  'norm_Þ',\n",
              "  'norm_¨',\n",
              "  'norm_µ',\n",
              "  'norm_´',\n",
              "  'norm_(',\n",
              "  'norm_á',\n",
              "  'norm_õ',\n",
              "  'norm_š',\n",
              "  'norm_Ù',\n",
              "  'norm_³',\n",
              "  'norm_’',\n",
              "  'norm_*',\n",
              "  'norm_ü',\n",
              "  'norm_è',\n",
              "  'norm_²',\n",
              "  'norm_Î',\n",
              "  'norm_Ñ',\n",
              "  'norm_œ',\n",
              "  'norm_=',\n",
              "  'norm_`',\n",
              "  \"norm_'\",\n",
              "  'norm_Å',\n",
              "  'norm_Á',\n",
              "  'norm_Ï',\n",
              "  'norm_Ê',\n",
              "  'norm_@',\n",
              "  'norm_ú',\n",
              "  'norm_Š',\n",
              "  'norm_•',\n",
              "  'norm_!',\n",
              "  'norm_ð',\n",
              "  'norm_×',\n",
              "  'norm_Ü',\n",
              "  'norm_ó',\n",
              "  'norm_æ',\n",
              "  'norm_É',\n",
              "  'norm_%',\n",
              "  'norm_à',\n",
              "  'norm_[',\n",
              "  'norm_Ž',\n",
              "  'norm_ä',\n",
              "  'norm_ý',\n",
              "  'norm_\\x92',\n",
              "  'norm_®',\n",
              "  'norm_¥',\n",
              "  'norm_î',\n",
              "  'norm_Ø',\n",
              "  'norm_±',\n",
              "  'norm_ÿ',\n",
              "  'norm_:',\n",
              "  'norm_|',\n",
              "  'norm_û',\n",
              "  'norm_ô',\n",
              "  'norm_·',\n",
              "  'norm_?',\n",
              "  'norm_&',\n",
              "  'norm_ö',\n",
              "  'norm_ê',\n",
              "  'norm_~',\n",
              "  'norm_÷',\n",
              "  'norm_ò',\n",
              "  'norm_È',\n",
              "  'norm_Ú',\n",
              "  'norm_/',\n",
              "  'norm__',\n",
              "  'norm_+',\n",
              "  'norm_ç',\n",
              "  'norm_Ó',\n",
              "  'norm_é',\n",
              "  'norm_ē',\n",
              "  'norm_†',\n",
              "  'norm_,',\n",
              "  'norm_#',\n",
              "  'norm_)',\n",
              "  'norm_©',\n",
              "  'norm_Û',\n",
              "  'norm_Ä',\n",
              "  'norm_„',\n",
              "  'norm_ì',\n",
              "  'norm_Æ',\n",
              "  'norm_À',\n",
              "  'norm_-',\n",
              "  'norm_°',\n",
              "  'norm_Â',\n",
              "  'norm_ñ',\n",
              "  'norm_$',\n",
              "  'norm_ã',\n",
              "  'pos_tag_tokens',\n",
              "  'num_pos_tokens',\n",
              "  'lemma_text'],\n",
              " 'feature_importances': array([0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.03199066])}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_text_data = pd.read_pickle(base_path)\n",
        "new_text_data = new_text_data.drop(columns=['norm_.'])\n",
        "test = info_gathering_repeat(new_text_data)\n",
        "test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8o9fDfD7RXY",
        "outputId": "a9fcf1a9-381b-45cd-d6e1-b0665167821b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train_acc': 0.7086523736600306,\n",
              " 'train_precision': 0.7032514012226285,\n",
              " 'train_recall': 0.7205971703845152,\n",
              " 'train_f1': 0.7118186300077881,\n",
              " 'test_acc': 0.6999479904071195,\n",
              " 'test_precision': 0.6916835122284543,\n",
              " 'test_recall': 0.7096453817841911,\n",
              " 'test_f1': 0.7005493317184999,\n",
              " 'train_roc_auc_value': 0.7862380046026203,\n",
              " 'test_roc_auc_value': 0.770534602780415,\n",
              " 'columns_in_data': ['original_text',\n",
              "  'label',\n",
              "  'closed_parentheses',\n",
              "  'AoA_score',\n",
              "  'Brysbaert_score',\n",
              "  'd_c_norm_1',\n",
              "  'num_toks_1',\n",
              "  'avg_tok_len_1',\n",
              "  'max_tok_len_1',\n",
              "  'num_char_norm_1',\n",
              "  'non_alphanumeric_1',\n",
              "  'num_non_ws_char',\n",
              "  'norm_<',\n",
              "  'norm_ß',\n",
              "  'norm_Ð',\n",
              "  'norm_º',\n",
              "  'norm_ù',\n",
              "  'norm_™',\n",
              "  'norm_Ò',\n",
              "  'norm_Ç',\n",
              "  'norm_ë',\n",
              "  'norm_å',\n",
              "  'norm_ø',\n",
              "  'norm_>',\n",
              "  'norm_Ö',\n",
              "  'norm_ž',\n",
              "  'norm_Õ',\n",
              "  'norm_Í',\n",
              "  'norm_ï',\n",
              "  'norm_Ë',\n",
              "  'norm_Ã',\n",
              "  'norm_â',\n",
              "  'norm_Ì',\n",
              "  'norm_í',\n",
              "  'norm_Ô',\n",
              "  'norm_þ',\n",
              "  'norm_§',\n",
              "  'norm_¹',\n",
              "  'norm_Þ',\n",
              "  'norm_¨',\n",
              "  'norm_µ',\n",
              "  'norm_´',\n",
              "  'norm_(',\n",
              "  'norm_á',\n",
              "  'norm_õ',\n",
              "  'norm_š',\n",
              "  'norm_Ù',\n",
              "  'norm_³',\n",
              "  'norm_’',\n",
              "  'norm_*',\n",
              "  'norm_ü',\n",
              "  'norm_è',\n",
              "  'norm_²',\n",
              "  'norm_Î',\n",
              "  'norm_Ñ',\n",
              "  'norm_œ',\n",
              "  'norm_=',\n",
              "  'norm_`',\n",
              "  \"norm_'\",\n",
              "  'norm_Å',\n",
              "  'norm_Á',\n",
              "  'norm_Ï',\n",
              "  'norm_;',\n",
              "  'norm_Ê',\n",
              "  'norm_@',\n",
              "  'norm_ú',\n",
              "  'norm_Š',\n",
              "  'norm_•',\n",
              "  'norm_!',\n",
              "  'norm_ð',\n",
              "  'norm_×',\n",
              "  'norm_Ü',\n",
              "  'norm_ó',\n",
              "  'norm_æ',\n",
              "  'norm_É',\n",
              "  'norm_%',\n",
              "  'norm_à',\n",
              "  'norm_[',\n",
              "  'norm_Ž',\n",
              "  'norm_ä',\n",
              "  'norm_ý',\n",
              "  'norm_\\x92',\n",
              "  'norm_®',\n",
              "  'norm_¥',\n",
              "  'norm_î',\n",
              "  'norm_Ø',\n",
              "  'norm_±',\n",
              "  'norm_ÿ',\n",
              "  'norm_:',\n",
              "  'norm_|',\n",
              "  'norm_û',\n",
              "  'norm_ô',\n",
              "  'norm_·',\n",
              "  'norm_?',\n",
              "  'norm_&',\n",
              "  'norm_ö',\n",
              "  'norm_ê',\n",
              "  'norm_~',\n",
              "  'norm_÷',\n",
              "  'norm_ò',\n",
              "  'norm_È',\n",
              "  'norm_Ú',\n",
              "  'norm_/',\n",
              "  'norm__',\n",
              "  'norm_+',\n",
              "  'norm_ç',\n",
              "  'norm_Ó',\n",
              "  'norm_é',\n",
              "  'norm_ē',\n",
              "  'norm_†',\n",
              "  'norm_,',\n",
              "  'norm_#',\n",
              "  'norm_)',\n",
              "  'norm_©',\n",
              "  'norm_Û',\n",
              "  'norm_Ä',\n",
              "  'norm_„',\n",
              "  'norm_ì',\n",
              "  'norm_Æ',\n",
              "  'norm_À',\n",
              "  'norm_-',\n",
              "  'norm_°',\n",
              "  'norm_Â',\n",
              "  'norm_ñ',\n",
              "  'norm_$',\n",
              "  'norm_ã',\n",
              "  'pos_tag_tokens',\n",
              "  'num_pos_tokens',\n",
              "  'lemma_text'],\n",
              " 'feature_importances': array([0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.03315886])}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_ablation_df.to_csv('/content/drive/My Drive/feature_ablation_df.csv')"
      ],
      "metadata": {
        "id": "lJV_qovQ7Rhs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}